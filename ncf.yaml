args:
  training_args:
    train_data: ./data/data_train.csv
    sample_data: ./data/sampleSubmission.csv
    num_users: 10000
    num_items: 1000
    test_size: 0.1
    random_seed: 42
    device: "cpu"
    min_rate: 1
    max_rate: 5

  experiment_args:
    model_name: "ncf"
    model_instance_name: "ncf"
    generate_submissions: True
    submission_folder: ./output/submission
    save_full_pred: False # True: for cross validation
    verbose: True

  svd_args:
    imputation: "zero"
    rank: 9
  als_args:
    imputation: "zero"
    num_iterations: 5
    reg_param: 0.3
    latent_dim: 3
  isvd_args:
    imputation: "mean"
    num_iterations: 15
    type: "nnr" # svp: Singular Value Projection, nnr: Nuclear Norm Relaxation
    eta: 0.3
    rank: 9
    shrinkage: 37
  svdpp_args:
    n_factors: 3
    lr_all: 0.003
    n_epochs: 100
    reg_all: 0.05
  bfm_args:
    algorithm: "oprobit" # regression, oprobit
    variational: False
    iteration: 1000 # 500 # 512
    dimension: 16
    use_iu: True
    use_ii: True
  ncf_args:
    EPOCHS: 20 # number of epochs
    BATCH_SIZE: 128 # batch size
    n_factors: 3 # embedding dimensions for the GMF layer
    learning_rate: 0.001 # learning rate
    train_file: ./data/actual.csv # intermediate location for saving the training dataset ()
  vae_args:
    num_iterations: 1000
    batch_size: 1024
    hidden_dim: 256
    latent_dim: 32
    dropout: 0.5
    lr: 0.025
    weight_decay: 0
    gamma: 0.997
    beta: 0.2
  ensemble_args:
    fold_number: 5
    shuffle: True
    regressor: 'GradientBoost'
    data_ensemble: './output/data_ensemble/'
    models: ["bfm_op_rk32_iter1000_cv10", "bfm_reg_rk16_iter1000_cv10"] # "bfm_reg_var_500_32", "bfm_op_novar_500_32", "bfm_reg_novar_500_32"
  cv_args:
    weight_entries: False # whether to do weighted (ensemble) sampling
    sample_proportion: 0.6 # proportion of training data sampled for each fold
    full_pred_provided: False
    full_pred_fn: "bfm_op_rk32_iter1000"